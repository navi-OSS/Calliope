{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ’Ž Bicameral Surgery: Gemma-3 (English Pruned)\n",
                "\n",
                "**Strategy**: Instead of training from scratch, we perform "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Authentication\n",
                "Gemma-3 requires a Hugging Face token."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from google.colab import userdata\n",
                "\n",
                "# Set your HF_TOKEN in Colab Secrets\n",
                "try:\n",
                "    os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
                "except:\n",
                "    print(\"âš ï¸ Please set 'HF_TOKEN' in Colab Secrets or paste it below:\")\n",
                "    # os.environ[\"HF_TOKEN\"] = \"your_token_here\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import gc\n",
                "\n",
                "def cleanup():\n",
                "    gc.collect()\n",
                "    torch.cuda.empty_cache()\n",
                "    print(\"ðŸ§¹ GPU Memory Cleared\")\n",
                "\n",
                "cleanup()\n",
                "\n",
                "%pip install -q torch transformers datasets tqdm wandb accelerate"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load and Prune Gemma-3\n",
                "We strip the 256k vocabulary down to ~64k English-centric tokens."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
                "from surgery.prune import find_english_tokens, prune_gemma_model\n",
                "\n",
                "MODEL_ID = \"google/gemma-3-270m\"\n",
                "\n",
                "print(\"ðŸ“¥ Loading Base Gemma-3...\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
                "base_model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_ID, \n",
                "    torch_dtype=torch.bfloat16, \n",
                "    device_map=\"auto\"\n",
                ")\n",
                "\n",
                "print(\"âœ‚ï¸ Pruning Multilingual Tokens...\")\n",
                "keep_indices = find_english_tokens(tokenizer, max_vocab_size=65536)\n",
                "model = prune_gemma_model(base_model, keep_indices)\n",
                "\n",
                "print(f\"âœ… Pruning Complete. New Vocab Size: {model.config.vocab_size}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Perform Bicameral Surgery\n",
                "Inject the Symbolic Lobe into the pre-trained blocks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from bicameral.config import BicameralConfig\n",
                "from bicameral.surgery_model import BicameralSurgery\n",
                "\n",
                "# Match Gemma-3 dims\n",
                "config = BicameralConfig(\n",
                "    d_model=model.config.hidden_size, # Should be 1024 for 270M\n",
                "    n_roles=32,                       # Increased for reasoning\n",
                "    d_filler=64,\n",
                "    gate_init_bias=-2.0,              # Start with pure Gemma fluency\n",
                "    vocab_size=model.config.vocab_size\n",
                ")\n",
                "\n",
                "model = BicameralSurgery(model, config)\n",
                "print(f\"ðŸ§  Surgery Successful!\")\n",
                "print(f\"Total Params: {model.count_total_parameters()/1e6:.1f}M\")\n",
                "print(f\"Trainable (Symbolic) Params: {model.count_trainable_parameters()/1e6:.1f}M\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Logic Training (bAbI)\n",
                "Focus compute entirely on reasoning induction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from data.dataset import LMDataset\n",
                "from torch.utils.data import DataLoader\n",
                "from torch.amp import GradScaler, autocast\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "print(\"ðŸ“¦ Loading Logic Dataset (bAbI)...\")\n",
                "# We use the pruned model's vocab size but same tokenizer logic\n",
                "ds_babi = LMDataset(tokenizer, \"Muennighoff/babi\", split=\"train\", streaming=False)\n",
                "loader = DataLoader(ds_babi, batch_size=8, shuffle=True)\n",
                "\n",
                "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
                "scaler = GradScaler('cuda')\n",
                "ORTHO_WEIGHT = 0.001\n",
                "ACCUMULATION = 8\n",
                "\n",
                "model.train()\n",
                "pbar = tqdm(total=2000, desc=\"Logic Injection\")\n",
                "\n",
                "for step, batch in enumerate(loader):\n",
                "    if step >= 2000: break\n",
                "    \n",
                "    input_ids = batch['input_ids'].to(\"cuda\")\n",
                "    \n",
                "    with autocast('cuda'):\n",
                "        out = model(input_ids, labels=input_ids)\n",
                "        ortho_loss = model.get_orthogonality_loss() * ORTHO_WEIGHT\n",
                "        loss = (out.loss + ortho_loss) / ACCUMULATION\n",
                "    \n",
                "    scaler.scale(loss).backward()\n",
                "    \n",
                "    if (step + 1) % ACCUMULATION == 0:\n",
                "        scaler.step(optimizer)\n",
                "        scaler.update()\n",
                "        optimizer.zero_grad()\n",
                "        pbar.set_postfix({'loss': f\"{loss.item()*ACCUMULATION:.4f}\"})\n",
                "        pbar.update(1)\n",
                "        \n",
                "print(\"âœ¨ Logic Training Complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Final Reason-Fluency Test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.eval()\n",
                "prompt = \"John went to the kitchen. John picked up the ball. John went to the garden. John dropped the ball. Where is the ball?\"\n",
                "\n",
                "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
                "with torch.no_grad():\n",
                "    gen = model.gemma.generate(**inputs, max_new_tokens=20)\n",
                "    \n",
                "print(f\"\\nPrompt: {prompt}\")\n",
                "print(f\"Gen: {tokenizer.decode(gen[0])}\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}